import yaml
import torch
from pathlib import Path
from datasets import load_dataset, concatenate_datasets
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType

# ğŸ”§ config íŒŒì¼ ë¡œë“œ
with open("configs/tinyllama.yaml", "r") as f:
    cfg = yaml.safe_load(f)

# ğŸ“¦ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €
model_id = cfg["model"]["base_model"]
tokenizer = AutoTokenizer.from_pretrained(cfg["tokenizer"]["tokenizer_name"])

# í† í¬ë‚˜ì´ì € ì„¤ì • ì ìš©
tokenizer.padding_side = cfg["tokenizer"]["padding_side"]
tokenizer.truncation_side = cfg["tokenizer"]["truncation_side"]

# íŒ¨ë”© í† í°ì´ ì—†ìœ¼ë©´ EOS í† í°ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì‚¬ìš©
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if cfg["training"]["fp16"] else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# LoRA ì„¤ì • ì ìš©
if cfg["model"]["use_lora"]:
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=cfg["model"]["lora_r"],
        lora_alpha=cfg["model"]["lora_alpha"],
        lora_dropout=cfg["model"]["lora_dropout"],
        target_modules=cfg["model"]["lora_target_modules"],
        bias="none",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

# ğŸ“ JSONL ë°ì´í„°ì…‹ ë¡œë”©
jsonl_dir = Path(cfg["dataset"]["train_jsonl_dir"])
jsonl_files = list(jsonl_dir.glob("*.jsonl"))
assert jsonl_files, f"No JSONL files found in {jsonl_dir}"

print(f"Found {len(jsonl_files)} JSONL files")

datasets = [load_dataset("json", data_files=str(f), split="train") for f in jsonl_files]
dataset = concatenate_datasets(datasets)

print(f"Total dataset size: {len(dataset)}")

# ğŸ“„ í…ìŠ¤íŠ¸ í¬ë§· í•¨ìˆ˜ - ì‹¤ì œ ë°ì´í„° í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
def format_record(example):
    # ì‹¤ì œ ë°ì´í„°ëŠ” text í•„ë“œì— ìˆìŒ
    if "text" in example:
        return example["text"]
    elif "messages" in example:
        return tokenizer.apply_chat_template(example["messages"], tokenize=False)
    elif "input" in example and "output" in example:
        return f"{example['input']}\n###\n{example['output']}"
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤. Available keys: {list(example.keys())}")

# ë°ì´í„° ìƒ˜í”Œ í™•ì¸
print("Sample data format:")
sample = dataset[0]
print(f"Keys: {list(sample.keys())}")
print(f"Text preview: {sample.get('text', 'N/A')[:100]}...")

dataset = dataset.map(lambda x: {"text": format_record(x)})

# âœ‚ï¸ í† í¬ë‚˜ì´ì§•
tokenized_dataset = dataset.map(
    lambda x: tokenizer(
        x["text"],
        padding="max_length",
        truncation=True,
        max_length=cfg["dataset"]["max_seq_length"],
        return_tensors=None  # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ Noneìœ¼ë¡œ ì„¤ì •
    ),
    batched=True,
    remove_columns=dataset.column_names,
)

print(f"Tokenized dataset size: {len(tokenized_dataset)}")

# ğŸ› ï¸ í•™ìŠµ ì„¤ì •
args = TrainingArguments(
    output_dir=cfg["training"]["output_dir"],
    per_device_train_batch_size=int(cfg["training"]["per_device_train_batch_size"]),
    gradient_accumulation_steps=int(cfg["training"]["gradient_accumulation_steps"]),
    num_train_epochs=float(cfg["training"]["num_train_epochs"]),
    learning_rate=float(cfg["training"]["learning_rate"]),
    weight_decay=float(cfg["training"]["weight_decay"]),
    warmup_steps=int(cfg["training"]["warmup_steps"]),
    lr_scheduler_type=cfg["training"]["lr_scheduler_type"],
    fp16=bool(cfg["training"]["fp16"]),
    logging_dir="./logs",
    logging_steps=int(cfg["training"]["logging_steps"]),
    save_steps=int(cfg["training"]["save_steps"]),
    save_total_limit=int(cfg["training"]["save_total_limit"]),
    # evaluation_strategy=cfg["training"]["evaluation_strategy"],  # í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìœ¼ë¯€ë¡œ ì œê±°
    dataloader_num_workers=int(cfg["training"]["dataloader_num_workers"]),
    seed=int(cfg["training"]["seed"]),
    report_to="none",
    remove_unused_columns=False,  # PEFT ëª¨ë¸ì„ ìœ„í•´ í•„ìš”
)


# ğŸ“¦ Trainer
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
)

# ğŸš€ Train
if __name__ == "__main__":
    print("ğŸš€ Start training...")
    print(f"Model: {model_id}")
    print(f"Dataset size: {len(dataset)}")
    print(f"Max sequence length: {cfg['dataset']['max_seq_length']}")
    print(f"Batch size: {cfg['training']['per_device_train_batch_size']}")
    print(f"Gradient accumulation: {cfg['training']['gradient_accumulation_steps']}")
    print(f"Effective batch size: {cfg['training']['per_device_train_batch_size'] * cfg['training']['gradient_accumulation_steps']}")
    
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    if cfg["model"]["use_lora"]:
        trainer.save_model()
        print("âœ… LoRA model saved.")
    else:
        trainer.save_model()
        print("âœ… Full model saved.")
    
    print("âœ… Training complete.")
